# Release v0.6.5 - Enhanced Transcription & Configuration

**Release Date**: January 29, 2026  
**Status**: Previous Release  
**Type**: Major Feature Release with Breaking Changes

---

## Overview

Version 0.6.5 introduces a comprehensive configuration management system and multiple transcription engine support, transforming the Content Pipeline into a flexible, enterprise-ready transcription platform. This release includes breaking changes that require migration from v0.6.0.

---

## What's New

### 1. Multiple Transcription Engines

The pipeline now supports three transcription engines with explicit selection:

- **local-whisper**: Privacy-first local processing using OpenAI Whisper
- **openai-whisper**: Cloud-based transcription via OpenAI API
- **aws-transcribe**: Enterprise-grade transcription via AWS Transcribe
- **auto**: Intelligent engine selection with fallback support

### 2. Configuration Management System

Hierarchical configuration system with multiple sources:

- **CLI Flags**: Highest priority (e.g., `--engine`, `--output-dir`)
- **Environment Variables**: Service credentials and overrides
- **Project Config**: `./.content-pipeline/config.yaml`
- **User Config**: `~/.content-pipeline/config.yaml`
- **System Defaults**: Built-in fallback values

### 3. Flexible Output Management

Output paths are now fully configurable:

- Configure via `--output-dir` flag
- Set in configuration files
- Override with `CONTENT_PIPELINE_OUTPUT_DIR` environment variable
- No longer hardcoded to `./output/`

### 4. Enhanced Testing Infrastructure

- Comprehensive testing guide with execution strategies
- Test isolation fixtures for reliable test execution
- Test markers for slow and external tests
- Property-based testing for configuration validation

---

## Breaking Changes

### ⚠️ Required Engine Selection

**Impact**: All transcription commands now require explicit engine selection.

**Before (v0.6.0)**:
```bash
content-pipeline transcribe --source audio.mp3
```

**After (v0.6.5)**:
```bash
content-pipeline transcribe --engine local-whisper --source audio.mp3
```

**Migration**:
- Add `--engine` flag to all transcribe commands
- Or set default engine in configuration file
- Or use `--engine auto` for automatic selection

### ⚠️ Engine Name Changes

**Impact**: Engine names have been standardized to provider-first pattern.

| Old Name (v0.6.0) | New Name (v0.6.5) |
|-------------------|-------------------|
| `whisper-local`   | `local-whisper`   |
| `whisper-api`     | `openai-whisper`  |
| N/A               | `aws-transcribe`  |

**Migration**:
- Update any scripts or configurations using old engine names
- Backward compatibility maintained via adapter wrapper

### ⚠️ Output Directory Changes

**Impact**: Output directory is no longer hardcoded to `./output/`.

**Before (v0.6.0)**:
- All output automatically saved to `./output/`

**After (v0.6.5)**:
- Must specify output directory via flag, config, or environment variable
- Default behavior: current directory

**Migration**:
```bash
# Option 1: Use CLI flag
content-pipeline transcribe --engine local-whisper --output-dir ./output --source audio.mp3

# Option 2: Set in config file
echo "output_dir: ./output" >> .content-pipeline/config.yaml

# Option 3: Set environment variable
export CONTENT_PIPELINE_OUTPUT_DIR=./output
```

---

## New Features

### Configuration Files

Create configuration files for project-wide or user-specific settings:

**Project Configuration** (`./.content-pipeline/config.yaml`):
```yaml
engine: local-whisper
output_dir: ./transcripts
log_level: info

whisper_local:
  model: medium
  device: auto
```

**User Configuration** (`~/.content-pipeline/config.yaml`):
```yaml
engine: openai-whisper
log_level: debug

whisper_api:
  api_key: ${OPENAI_API_KEY}
```

### Environment Variables

Configure via environment variables:

```bash
# Service credentials
export OPENAI_API_KEY="your-api-key"
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"

# Pipeline configuration
export CONTENT_PIPELINE_DEFAULT_ENGINE="local-whisper"
export CONTENT_PIPELINE_OUTPUT_DIR="./transcripts"
export CONTENT_PIPELINE_LOG_LEVEL="info"
```

### Auto-Selection Engine

Intelligent engine selection with fallback:

```bash
content-pipeline transcribe --engine auto --source audio.mp3
```

The auto-selector will:
1. Check available engines (API keys, dependencies)
2. Select best available engine based on configuration
3. Fallback to alternative engines if primary fails
4. Provide detailed logging of selection process

---

## Upgrade Guide

### Step 1: Update Installation

```bash
pip install --upgrade content-pipeline
```

### Step 2: Verify Installation

```bash
content-pipeline --version
# Should show: content-pipeline, version 0.6.5
```

### Step 3: Update Scripts

Add `--engine` flag to all transcribe commands:

```bash
# Old command (v0.6.0)
content-pipeline transcribe --source audio.mp3

# New command (v0.6.5)
content-pipeline transcribe --engine local-whisper --source audio.mp3
```

### Step 4: Configure Output Directory

Choose one of these options:

```bash
# Option 1: CLI flag (recommended for scripts)
content-pipeline transcribe --engine local-whisper --output-dir ./output --source audio.mp3

# Option 2: Configuration file (recommended for projects)
mkdir -p .content-pipeline
echo "output_dir: ./output" > .content-pipeline/config.yaml

# Option 3: Environment variable (recommended for CI/CD)
export CONTENT_PIPELINE_OUTPUT_DIR=./output
```

### Step 5: Test Configuration

```bash
# Test with auto-selection
content-pipeline transcribe --engine auto --source test.mp3

# Verify output location
ls -la ./output/  # or your configured output directory
```

---

## Configuration Examples

### Privacy-Focused Setup

```yaml
engine: local-whisper
output_dir: ./transcripts

whisper_local:
  model: medium
  device: auto

auto_prefer_local: true
auto_fallback_enabled: false
```

### Quality-Focused Setup

```yaml
engine: openai-whisper
output_dir: ./transcripts

whisper_api:
  api_key: ${OPENAI_API_KEY}
  temperature: 0.0

auto_prefer_local: false
```

### Enterprise Setup

```yaml
engine: aws-transcribe
output_dir: /var/transcripts
log_level: warning

logging:
  enable_file_logging: true
  log_file: /var/log/content-pipeline.log

monitoring:
  enable_metrics: true
```

---

## Testing

### Run Fast Tests

```bash
# Recommended for development
pytest -m "not slow and not external"
```

### Run All Tests

```bash
# Full test suite
pytest tests/
```

### Test Markers

- `slow`: Tests that take 5+ minutes (CLI workflows)
- `external`: Tests requiring external services (YouTube, APIs)
- `integration`: End-to-end workflow tests

---

## Known Issues

### 1. YouTube Extraction Rate Limiting

**Issue**: YouTube may return 403 Forbidden errors for some videos.

**Workaround**: Use local audio files or retry after delay.

**Status**: External dependency limitation.

### 2. Whisper Model Download

**Issue**: First run of local-whisper downloads large model files.

**Workaround**: Pre-download models or use smaller model sizes.

**Status**: Expected behavior.

---

## Deprecations

### Backward Compatibility Adapter

The `whisper.py` adapter provides backward compatibility but is deprecated:

```python
# Deprecated (still works)
from pipeline.transcribers.adapters.whisper import WhisperAdapter

# Recommended
from pipeline.transcribers.adapters.local_whisper import LocalWhisperAdapter
```

**Timeline**: Will be removed in v0.7.0.

---

## Performance Notes

### Engine Performance Comparison

| Engine | Speed | Quality | Privacy | Cost |
|--------|-------|---------|---------|------|
| local-whisper | Medium | High | Excellent | Free |
| openai-whisper | Fast | Excellent | Low | Pay-per-use |
| aws-transcribe | Fast | Excellent | Medium | Pay-per-use |

### Recommendations

- **Development**: Use `local-whisper` with `tiny` model for speed
- **Production**: Use `openai-whisper` or `aws-transcribe` for quality
- **Privacy-sensitive**: Use `local-whisper` with `medium` or `large` model

---

## Security Considerations

### API Key Management

**Never hardcode API keys** in configuration files:

```yaml
# ❌ BAD - Hardcoded key
whisper_api:
  api_key: "sk-1234567890abcdef"

# ✅ GOOD - Environment variable
whisper_api:
  api_key: ${OPENAI_API_KEY}
```

### File Permissions

Set appropriate permissions on configuration files:

```bash
chmod 600 ~/.content-pipeline/config.yaml
chmod 600 ./.content-pipeline/config.yaml
```

---

## Documentation

- **[Installation Guide](../installation-guide.md)**: Setup and installation
- **[CLI Commands](../cli-commands.md)**: Command-line reference
- **[Architecture](../architecture.md)**: System design
- **[Testing Guide](../testing-guide.md)**: Testing strategies

---

## Support

For issues, questions, or feedback:

- **GitHub Issues**: Report bugs and feature requests
- **Documentation**: Check docs/ folder for guides
- **CHANGELOG.md**: Quick reference for all changes

---

## Next Release

**v0.7.0** - Released January 30, 2026

See [v0.7.0.md](v0.7.0.md) for details on LLM-powered enrichment features.
