# Content Pipeline Configuration v0.6.5
# This file configures transcription engines and output settings for your project
#
# Configuration Precedence (highest to lowest):
# 1. CLI flags (--engine, --output-dir, etc.)
# 2. Environment variables (CONTENT_PIPELINE_*, OPENAI_API_KEY)
# 3. Project config (./.content-pipeline/config.yaml) - THIS FILE
# 4. User config (~/.content-pipeline/config.yaml)
# 5. System defaults

# =============================================================================
# CORE SETTINGS
# =============================================================================

# Default transcription engine to use
# Options: local-whisper, openai-whisper, auto
# - local-whisper: Process audio locally (privacy-focused, requires installation)
# - openai-whisper: Use OpenAI Whisper API (highest quality, requires API key)
# - auto: Automatically select best available engine
engine: auto

# Default output directory for transcripts
# Supports environment variable substitution: ${VAR_NAME:-default_value}
# Examples:
#   output_dir: ./transcripts                    # Relative to current directory
#   output_dir: /home/user/transcripts          # Absolute path
#   output_dir: ${HOME}/Documents/transcripts   # Using environment variable
output_dir: ${CONTENT_PIPELINE_OUTPUT_DIR:-./transcripts}

# Default logging level
# Options: debug, info, warning, error
# - debug: Detailed execution information (useful for troubleshooting)
# - info: Standard progress information (recommended for normal use)
# - warning: Only warnings and errors
# - error: Only error messages
log_level: ${CONTENT_PIPELINE_LOG_LEVEL:-info}

# Default language hint for transcription (optional)
# Use ISO 639-1 language codes (e.g., 'en', 'es', 'fr', 'de')
# Set to null to let the engine auto-detect language
language: null

# =============================================================================
# ENGINE-SPECIFIC CONFIGURATIONS
# =============================================================================

# Local Whisper Engine Configuration
whisper_local:
  # Whisper model size to use
  # Options: tiny, base, small, medium, large
  # - tiny: Fastest, least accurate (~39 MB)
  # - base: Good balance of speed and accuracy (~74 MB)
  # - small: Better accuracy, slower (~244 MB)
  # - medium: High accuracy (~769 MB)
  # - large: Best accuracy, slowest (~1550 MB)
  model: ${WHISPER_LOCAL_MODEL:-base}
  
  # Device to use for processing
  # Options: cpu, cuda, auto
  # - cpu: Use CPU only (slower but works everywhere)
  # - cuda: Use GPU acceleration (faster, requires NVIDIA GPU)
  # - auto: Automatically detect best available device
  device: auto
  
  # Compute type for faster-whisper (if available)
  # Options: default, int8, int8_float16, int16, float16, float32
  compute_type: default
  
  # Operation timeout in seconds
  timeout: 300
  
  # Number of retry attempts on failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 1.0

# OpenAI Whisper API Configuration
whisper_api:
  # OpenAI API key (required for openai-whisper engine)
  # SECURITY: Use environment variable instead of hardcoding
  # Get your API key at: https://platform.openai.com/api-keys
  api_key: ${OPENAI_API_KEY:-}
  # OpenAI model to use
  # Currently only 'whisper-1' is available
  model: whisper-1
  
  # Temperature for transcription (0.0 to 1.0)
  # Lower values make output more focused and deterministic
  temperature: 0.0
  
  # Response format
  # Options: json, text, srt, verbose_json, vtt
  response_format: json
  
  # API request timeout in seconds
  timeout: 60
  
  # Number of retry attempts on API failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 2.0
  
  # Cost per minute in USD (for cost estimation)
  # Default: 0.006 (OpenAI's standard pricing as of 2024)
  # Override this if you have custom pricing or volume discounts
  # Can also be set via WHISPER_API_COST_PER_MINUTE environment variable
  cost_per_minute_usd: 0.006

# AWS Transcribe Configuration (if using aws-transcribe engine)
aws_transcribe:
  # AWS credentials (required for aws-transcribe engine)
  # SECURITY: Use environment variables instead of hardcoding
  # Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables
  # Or configure them here (not recommended for production)
  # Or use AWS CLI: Run 'aws configure' to set up credentials
  access_key_id: null
  secret_access_key: null
  
  # AWS region to use
  # Common regions: us-east-1, us-west-2, eu-west-1, ap-southeast-1
  region: ${AWS_DEFAULT_REGION:-us-east-1}
  
  # Language code for transcription
  # Use AWS language codes (e.g., 'en-US', 'es-ES', 'fr-FR', 'de-DE')
  # The system will auto-convert ISO 639-1 codes (en, es, fr) to AWS format
  language_code: en-US
  
  # Optional: Custom S3 bucket name for transcription files
  # If not specified, a default bucket will be created: content-pipeline-transcribe-{region}
  s3_bucket: null
  
  # Operation timeout in seconds (AWS Transcribe can take longer for large files)
  timeout: 600
  
  # Number of retry attempts on failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 2.0
  
  # Cost per minute in USD (for cost estimation)
  # Default: 0.024 (AWS standard pricing as of 2024)
  # Override this if you have custom pricing, volume discounts, or regional pricing
  # Can also be set via AWS_TRANSCRIBE_COST_PER_MINUTE environment variable
  cost_per_minute_usd: 0.024

# =============================================================================
# AUTO-SELECTION PREFERENCES
# =============================================================================

# Auto-selection engine preferences
# These settings control how the 'auto' engine chooses between available engines
auto_selection:
  # Prefer local processing when available
  # Set to false to prefer cloud services for potentially better quality
  prefer_local: true

  # Enable fallback to other engines if preferred engine fails
  # Set to false to fail immediately if preferred engine is unavailable
  fallback_enabled: true

# =============================================================================
# LLM CONFIGURATION (for enrichment workflows)
# =============================================================================

# LLM (Large Language Model) configuration for semantic enrichment
# Used by the 'enrich' command to generate summaries, tags, chapters, and highlights
# Supports multiple providers: Ollama (local), OpenAI, AWS Bedrock, Anthropic

llm:
  # Local Ollama Configuration
  # Ollama runs models locally on your machine (privacy-focused, zero cost)
  # Install Ollama from: https://ollama.ai
  ollama:
    # Base URL for Ollama API
    # Default: http://localhost:11434 (standard Ollama installation)
    # For remote Ollama: http://remote-host:11434
    base_url: ${OLLAMA_BASE_URL:-http://localhost:11434}
    
    # Default model to use if not specified in request
    # Popular models: llama2, llama2:13b, llama2:70b, mistral, mixtral
    # List available models: ollama list
    default_model: ${OLLAMA_MODEL:-llama2}
    
    # Maximum tokens for responses
    # Larger values allow longer outputs but take more time
    max_tokens: ${OLLAMA_MAX_TOKENS:-4096}
    
    # Sampling temperature (0.0 to 1.0)
    # Lower values (0.1-0.3): More focused and deterministic
    # Higher values (0.7-1.0): More creative and varied
    temperature: ${OLLAMA_TEMPERATURE:-0.3}
    
    # Request timeout in seconds
    # Local models may be slower, especially on CPU
    timeout: ${OLLAMA_TIMEOUT:-120}
  
  # OpenAI Configuration
  # OpenAI provides GPT models via cloud API (highest quality, requires API key)
  # Get your API key at: https://platform.openai.com/api-keys
  openai:
    # OpenAI API key (required for cloud-openai provider)
    # SECURITY: Use environment variable instead of hardcoding
    # Set OPENAI_API_KEY environment variable
    api_key: ${OPENAI_API_KEY:-}
    
    # Default model to use if not specified in request
    # Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo
    # - gpt-4: Best quality, highest cost
    # - gpt-4-turbo: Good balance of quality and speed
    # - gpt-3.5-turbo: Fastest, lowest cost
    default_model: ${OPENAI_MODEL:-gpt-4}
    
    # Maximum tokens for responses
    max_tokens: ${OPENAI_MAX_TOKENS:-4096}
    
    # Sampling temperature (0.0 to 1.0)
    temperature: ${OPENAI_TEMPERATURE:-0.7}
    
    # API request timeout in seconds
    timeout: ${OPENAI_TIMEOUT:-60}
  
  # AWS Bedrock Configuration
  # AWS Bedrock provides access to Claude and Titan models (enterprise-grade)
  # Requires AWS credentials and appropriate IAM permissions
  bedrock:
    # AWS region to use
    # Common regions: us-east-1, us-west-2, eu-west-1
    region: ${AWS_REGION:-us-east-1}
    
    # AWS credentials (optional if using IAM roles or AWS CLI configuration)
    # SECURITY: Use environment variables or IAM roles instead of hardcoding
    # Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables
    # Or configure them here (not recommended for production)
    access_key_id: ${AWS_ACCESS_KEY_ID:-}
    secret_access_key: ${AWS_SECRET_ACCESS_KEY:-}
    
    # AWS session token (optional, for temporary credentials)
    session_token: ${AWS_SESSION_TOKEN:-}
    
    # Default model to use if not specified in request
    # Options: amazon.nova-lite-v1:0, anthropic.claude-v2, anthropic.claude-3-sonnet
    # - amazon.nova-lite-v1:0: AWS native model, good balance
    # - anthropic.claude-v2: High quality, good for analysis
    # - anthropic.claude-3-sonnet: Latest Claude model, best quality
    default_model: ${BEDROCK_MODEL:-amazon.nova-lite-v1:0}
    
    # Maximum tokens for responses
    max_tokens: ${BEDROCK_MAX_TOKENS:-4096}
    
    # Sampling temperature (0.0 to 1.0)
    temperature: ${BEDROCK_TEMPERATURE:-0.7}
  
  # Anthropic Configuration
  # Anthropic provides Claude models via cloud API (high quality, requires API key)
  # Get your API key at: https://console.anthropic.com/
  anthropic:
    # Anthropic API key (required for cloud-anthropic provider)
    # SECURITY: Use environment variable instead of hardcoding
    # Set ANTHROPIC_API_KEY environment variable
    api_key: ${ANTHROPIC_API_KEY:-}
    
    # Default model to use if not specified in request
    # Options: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
    # - claude-3-opus: Best quality, highest cost
    # - claude-3-sonnet: Good balance of quality and speed
    # - claude-3-haiku: Fastest, lowest cost
    default_model: ${ANTHROPIC_MODEL:-claude-3-opus-20240229}
    
    # Maximum tokens for responses
    max_tokens: ${ANTHROPIC_MAX_TOKENS:-4096}
    
    # Sampling temperature (0.0 to 1.0)
    temperature: ${ANTHROPIC_TEMPERATURE:-0.7}
    
    # API request timeout in seconds
    timeout: ${ANTHROPIC_TIMEOUT:-60}

# =============================================================================
# EXAMPLES AND COMMON CONFIGURATIONS
# =============================================================================

# Example configurations for different use cases:

# Privacy-focused (local only):
# engine: local-whisper
# whisper_local:
#   model: medium
#   device: auto
# auto_selection:
#   prefer_local: true
#   fallback_enabled: false
# llm:
#   ollama:
#     base_url: http://localhost:11434
#     default_model: llama2:13b

# Quality-focused (cloud services):
# engine: openai-whisper
# whisper_api:
#   temperature: 0.0
# auto_selection:
#   prefer_local: false
# llm:
#   openai:
#     default_model: gpt-4
#     temperature: 0.7

# Development/testing:
# engine: auto
# log_level: debug
# whisper_local:
#   model: tiny  # Fastest for testing
# llm:
#   ollama:
#     default_model: llama2  # Fast local model for testing

# Enterprise (AWS):
# engine: aws-transcribe
# aws_transcribe:
#   region: us-east-1
# llm:
#   bedrock:
#     region: us-east-1
#     default_model: anthropic.claude-3-sonnet