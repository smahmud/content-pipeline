# Content Pipeline Configuration v0.6.5
# This file configures transcription engines and output settings for your project
#
# Configuration Precedence (highest to lowest):
# 1. CLI flags (--engine, --output-dir, etc.)
# 2. Environment variables (CONTENT_PIPELINE_*, OPENAI_API_KEY)
# 3. Project config (./.content-pipeline/config.yaml) - THIS FILE
# 4. User config (~/.content-pipeline/config.yaml)
# 5. System defaults

# =============================================================================
# CORE SETTINGS
# =============================================================================

# Default transcription engine to use
# Options: local-whisper, openai-whisper, auto
# - local-whisper: Process audio locally (privacy-focused, requires installation)
# - openai-whisper: Use OpenAI Whisper API (highest quality, requires API key)
# - auto: Automatically select best available engine
engine: auto

# Default output directory for transcripts
# Supports environment variable substitution: ${VAR_NAME:-default_value}
# Examples:
#   output_dir: ./transcripts                    # Relative to current directory
#   output_dir: /home/user/transcripts          # Absolute path
#   output_dir: ${HOME}/Documents/transcripts   # Using environment variable
output_dir: ${CONTENT_PIPELINE_OUTPUT_DIR:-./transcripts}

# Default logging level
# Options: debug, info, warning, error
# - debug: Detailed execution information (useful for troubleshooting)
# - info: Standard progress information (recommended for normal use)
# - warning: Only warnings and errors
# - error: Only error messages
log_level: ${CONTENT_PIPELINE_LOG_LEVEL:-info}

# Default language hint for transcription (optional)
# Use ISO 639-1 language codes (e.g., 'en', 'es', 'fr', 'de')
# Set to null to let the engine auto-detect language
language: null

# =============================================================================
# ENGINE-SPECIFIC CONFIGURATIONS
# =============================================================================

# Local Whisper Engine Configuration
whisper_local:
  # Whisper model size to use
  # Options: tiny, base, small, medium, large
  # - tiny: Fastest, least accurate (~39 MB)
  # - base: Good balance of speed and accuracy (~74 MB)
  # - small: Better accuracy, slower (~244 MB)
  # - medium: High accuracy (~769 MB)
  # - large: Best accuracy, slowest (~1550 MB)
  model: ${WHISPER_LOCAL_MODEL:-base}
  
  # Device to use for processing
  # Options: cpu, cuda, auto
  # - cpu: Use CPU only (slower but works everywhere)
  # - cuda: Use GPU acceleration (faster, requires NVIDIA GPU)
  # - auto: Automatically detect best available device
  device: auto
  
  # Compute type for faster-whisper (if available)
  # Options: default, int8, int8_float16, int16, float16, float32
  compute_type: default
  
  # Operation timeout in seconds
  timeout: 300
  
  # Number of retry attempts on failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 1.0

# OpenAI Whisper API Configuration
whisper_api:
  # OpenAI API key (required for openai-whisper engine)
  # SECURITY: Use environment variable instead of hardcoding
  # Get your API key at: https://platform.openai.com/api-keys
  api_key: ${OPENAI_API_KEY:-}
  # OpenAI model to use
  # Currently only 'whisper-1' is available
  model: whisper-1
  
  # Temperature for transcription (0.0 to 1.0)
  # Lower values make output more focused and deterministic
  temperature: 0.0
  
  # Response format
  # Options: json, text, srt, verbose_json, vtt
  response_format: json
  
  # API request timeout in seconds
  timeout: 60
  
  # Number of retry attempts on API failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 2.0

# AWS Transcribe Configuration (if using aws-transcribe engine)
aws_transcribe:
  # AWS credentials (required for aws-transcribe engine)
  # SECURITY: Use environment variables instead of hardcoding
  # Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables
  # Or configure them here (not recommended for production)
  # Or use AWS CLI: Run 'aws configure' to set up credentials
  access_key_id: null
  secret_access_key: null
  
  # AWS region to use
  # Common regions: us-east-1, us-west-2, eu-west-1, ap-southeast-1
  region: ${AWS_DEFAULT_REGION:-us-east-1}
  
  # Language code for transcription
  # Use AWS language codes (e.g., 'en-US', 'es-ES', 'fr-FR', 'de-DE')
  # The system will auto-convert ISO 639-1 codes (en, es, fr) to AWS format
  language_code: en-US
  
  # Optional: Custom S3 bucket name for transcription files
  # If not specified, a default bucket will be created: content-pipeline-transcribe-{region}
  s3_bucket: null
  
  # Operation timeout in seconds (AWS Transcribe can take longer for large files)
  timeout: 600
  
  # Number of retry attempts on failure
  retry_attempts: 3
  
  # Delay between retry attempts in seconds
  retry_delay: 2.0

# =============================================================================
# AUTO-SELECTION PREFERENCES
# =============================================================================

# Auto-selection engine preferences
# These settings control how the 'auto' engine chooses between available engines
auto_selection:
  # Prefer local processing when available
  # Set to false to prefer cloud services for potentially better quality
  prefer_local: true

  # Enable fallback to other engines if preferred engine fails
  # Set to false to fail immediately if preferred engine is unavailable
  fallback_enabled: true

# =============================================================================
# EXAMPLES AND COMMON CONFIGURATIONS
# =============================================================================

# Example configurations for different use cases:

# Privacy-focused (local only):
# engine: local-whisper
# whisper_local:
#   model: medium
#   device: auto
# auto_selection:
#   prefer_local: true
#   fallback_enabled: false

# Quality-focused (cloud services):
# engine: openai-whisper
# whisper_api:
#   temperature: 0.0
# auto_selection:
#   prefer_local: false

# Development/testing:
# engine: auto
# log_level: debug
# whisper_local:
#   model: tiny  # Fastest for testing