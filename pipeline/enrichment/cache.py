"""
Cache System

File-based caching system for enrichment results to prevent redundant
LLM API calls and reduce costs. Implements TTL expiration, size limits,
and cache key generation based on content hashing.
"""

import hashlib
import json
import os
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any

from pipeline.enrichment.schemas.enrichment_v1 import EnrichmentV1


@dataclass
class CacheEntry:
    """Cache entry with metadata.
    
    Attributes:
        key: Cache key (hash)
        result: Enrichment result
        created_at: Timestamp when cached
        expires_at: Timestamp when cache expires
        size_bytes: Size of cached data in bytes
    """
    key: str
    result: Dict[str, Any]
    created_at: float
    expires_at: float
    size_bytes: int


class CacheSystem:
    """File-based cache for enrichment results.
    
    This class implements a file-based caching system that:
    - Generates cache keys by hashing inputs
    - Stores enrichment results as JSON files
    - Implements TTL-based expiration
    - Enforces maximum cache size limits
    - Provides cache hit/miss statistics
    
    Cache files are stored in: .content-pipeline/cache/enrichment/
    
    Example:
        >>> cache = CacheSystem()
        >>> key = cache.generate_key(transcript, model, prompt, params)
        >>> result = cache.get(key)
        >>> if result is None:
        ...     result = perform_enrichment()
        ...     cache.set(key, result)
    """
    
    DEFAULT_CACHE_DIR = ".content-pipeline/cache/enrichment"
    DEFAULT_TTL_DAYS = 30
    DEFAULT_MAX_SIZE_MB = 500
    
    def __init__(
        self,
        cache_dir: Optional[str] = None,
        ttl_days: int = DEFAULT_TTL_DAYS,
        max_size_mb: int = DEFAULT_MAX_SIZE_MB
    ):
        """Initialize cache system.
        
        Args:
            cache_dir: Directory for cache files (default: .content-pipeline/cache/enrichment)
            ttl_days: Time-to-live in days (default: 30)
            max_size_mb: Maximum cache size in MB (default: 500)
        """
        self.cache_dir = Path(cache_dir or self.DEFAULT_CACHE_DIR)
        self.ttl_seconds = ttl_days * 24 * 60 * 60
        self.max_size_bytes = max_size_mb * 1024 * 1024
        
        # Create cache directory if it doesn't exist
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Statistics
        self.hits = 0
        self.misses = 0
    
    def generate_key(
        self,
        transcript_text: str,
        model: str,
        prompt_template: str,
        enrichment_types: list[str],
        parameters: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate cache key by hashing inputs.
        
        The cache key is generated by hashing:
        - Transcript text content
        - Model identifier
        - Prompt template
        - Enrichment types
        - Additional parameters (temperature, max_tokens, etc.)
        
        Args:
            transcript_text: The transcript text
            model: Model identifier
            prompt_template: Prompt template used
            enrichment_types: List of enrichment types
            parameters: Optional additional parameters
            
        Returns:
            Cache key (SHA256 hash)
        """
        # Create a deterministic string representation
        key_components = [
            transcript_text,
            model,
            prompt_template,
            "|".join(sorted(enrichment_types)),
        ]
        
        if parameters:
            # Sort parameters for deterministic hashing
            param_str = json.dumps(parameters, sort_keys=True)
            key_components.append(param_str)
        
        # Combine and hash
        combined = "\n".join(key_components)
        hash_obj = hashlib.sha256(combined.encode("utf-8"))
        return hash_obj.hexdigest()
    
    def get(self, key: str) -> Optional[EnrichmentV1]:
        """Retrieve cached result.
        
        Args:
            key: Cache key
            
        Returns:
            Cached EnrichmentV1 result or None if not found/expired
        """
        cache_file = self.cache_dir / f"{key}.json"
        
        # Check if cache file exists
        if not cache_file.exists():
            self.misses += 1
            return None
        
        try:
            # Load cache entry
            with open(cache_file, "r", encoding="utf-8") as f:
                entry_data = json.load(f)
            
            entry = CacheEntry(**entry_data)
            
            # Check if expired
            current_time = time.time()
            if current_time > entry.expires_at:
                # Remove expired entry
                cache_file.unlink()
                self.misses += 1
                return None
            
            # Cache hit!
            self.hits += 1
            
            # Reconstruct EnrichmentV1 from cached data
            result = EnrichmentV1(**entry.result)
            
            # Update metadata to indicate cache hit
            result.metadata.cache_hit = True
            
            return result
        
        except Exception as e:
            # If cache file is corrupted, remove it
            if cache_file.exists():
                cache_file.unlink()
            self.misses += 1
            return None
    
    def set(self, key: str, result: EnrichmentV1) -> bool:
        """Store result in cache.
        
        Args:
            key: Cache key
            result: Enrichment result to cache
            
        Returns:
            True if successfully cached, False otherwise
        """
        try:
            # Convert result to dict with JSON-serializable types
            result_dict = result.model_dump(mode='json')
            
            # Calculate size
            result_json = json.dumps(result_dict)
            size_bytes = len(result_json.encode("utf-8"))
            
            # Create cache entry
            current_time = time.time()
            entry = CacheEntry(
                key=key,
                result=result_dict,
                created_at=current_time,
                expires_at=current_time + self.ttl_seconds,
                size_bytes=size_bytes
            )
            
            # Check cache size limit before writing
            self._enforce_size_limit(size_bytes)
            
            # Write to cache file
            cache_file = self.cache_dir / f"{key}.json"
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(asdict(entry), f, indent=2, default=str)
            
            return True
        
        except Exception as e:
            # Log error but don't fail the enrichment operation
            return False
    
    def clear(self) -> int:
        """Clear all cache entries.
        
        Returns:
            Number of entries cleared
        """
        count = 0
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
            count += 1
        
        return count
    
    def clean_expired(self) -> int:
        """Remove expired cache entries.
        
        Returns:
            Number of entries removed
        """
        count = 0
        current_time = time.time()
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    entry_data = json.load(f)
                
                entry = CacheEntry(**entry_data)
                
                if current_time > entry.expires_at:
                    cache_file.unlink()
                    count += 1
            
            except Exception:
                # If file is corrupted, remove it
                cache_file.unlink()
                count += 1
        
        return count
    
    def get_size(self) -> int:
        """Get current cache size in bytes.
        
        Returns:
            Total size of all cache files in bytes
        """
        total_size = 0
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                total_size += cache_file.stat().st_size
            except Exception:
                pass
        
        return total_size
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics.
        
        Returns:
            Dict with cache statistics
        """
        total_requests = self.hits + self.misses
        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0.0
        
        # Count entries
        entry_count = len(list(self.cache_dir.glob("*.json")))
        
        # Get size
        size_bytes = self.get_size()
        size_mb = size_bytes / (1024 * 1024)
        
        return {
            "hits": self.hits,
            "misses": self.misses,
            "total_requests": total_requests,
            "hit_rate_percent": hit_rate,
            "entry_count": entry_count,
            "size_bytes": size_bytes,
            "size_mb": size_mb,
            "max_size_mb": self.max_size_bytes / (1024 * 1024),
            "ttl_days": self.ttl_seconds / (24 * 60 * 60)
        }
    
    def _enforce_size_limit(self, new_entry_size: int) -> None:
        """Enforce maximum cache size limit.
        
        If adding a new entry would exceed the size limit, removes
        oldest entries until there's enough space.
        
        Args:
            new_entry_size: Size of new entry to be added
        """
        current_size = self.get_size()
        
        # Check if we need to free space
        if current_size + new_entry_size <= self.max_size_bytes:
            return
        
        # Get all cache files with their creation times
        cache_files = []
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    entry_data = json.load(f)
                
                entry = CacheEntry(**entry_data)
                cache_files.append((cache_file, entry.created_at, entry.size_bytes))
            
            except Exception:
                # If file is corrupted, mark for removal
                cache_files.append((cache_file, 0, 0))
        
        # Sort by creation time (oldest first)
        cache_files.sort(key=lambda x: x[1])
        
        # Remove oldest entries until we have enough space
        space_needed = (current_size + new_entry_size) - self.max_size_bytes
        space_freed = 0
        
        for cache_file, _, size in cache_files:
            if space_freed >= space_needed:
                break
            
            try:
                cache_file.unlink()
                space_freed += size
            except Exception:
                pass
